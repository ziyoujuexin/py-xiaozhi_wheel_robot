#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å”¤é†’è¯è‡ªåŠ¨ç”Ÿæˆå·¥å…·

åŠŸèƒ½ï¼š
1. è¾“å…¥ä¸­æ–‡è‡ªåŠ¨è½¬æ¢ä¸ºå¸¦å£°è°ƒæ‹¼éŸ³
2. æŒ‰å­—æ¯åˆ†éš”æ‹¼éŸ³ï¼ˆå£°æ¯+éŸµæ¯ï¼‰
3. éªŒè¯tokenæ˜¯å¦åœ¨tokens.txtä¸­
4. è‡ªåŠ¨ç”Ÿæˆkeywords.txtæ ¼å¼
"""

import sys
from pathlib import Path

try:
    from pypinyin import lazy_pinyin, Style
except ImportError:
    print("âŒ ç¼ºå°‘ä¾èµ–: pypinyin")
    print("è¯·å®‰è£…: pip install pypinyin")
    sys.exit(1)


class KeywordGenerator:
    def __init__(self, model_dir: Path):
        """
        åˆå§‹åŒ–å”¤é†’è¯ç”Ÿæˆå™¨

        Args:
            model_dir: æ¨¡å‹ç›®å½•è·¯å¾„ï¼ˆåŒ…å«tokens.txtå’Œkeywords.txtï¼‰
        """
        self.model_dir = Path(model_dir)
        self.tokens_file = self.model_dir / "tokens.txt"
        self.keywords_file = self.model_dir / "keywords.txt"

        # åŠ è½½å·²æœ‰çš„tokens
        self.available_tokens = self._load_tokens()

        # å£°æ¯è¡¨ï¼ˆéœ€è¦åˆ†ç¦»çš„ï¼‰
        self.initials = [
            'b', 'p', 'm', 'f', 'd', 't', 'n', 'l',
            'g', 'k', 'h', 'j', 'q', 'x',
            'zh', 'ch', 'sh', 'r', 'z', 'c', 's', 'y', 'w'
        ]

    def _load_tokens(self) -> set:
        """åŠ è½½tokens.txtä¸­çš„æ‰€æœ‰å¯ç”¨token"""
        if not self.tokens_file.exists():
            print(f"âš ï¸  è­¦å‘Š: tokensæ–‡ä»¶ä¸å­˜åœ¨: {self.tokens_file}")
            return set()

        tokens = set()
        with open(self.tokens_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    # æ ¼å¼: "token id" æˆ– "token"
                    parts = line.split()
                    if parts:
                        tokens.add(parts[0])

        print(f"âœ… åŠ è½½äº† {len(tokens)} ä¸ªå¯ç”¨tokens")
        return tokens

    def _split_pinyin(self, pinyin: str) -> list:
        """
        å°†æ‹¼éŸ³æŒ‰å£°æ¯éŸµæ¯åˆ†éš”

        ä¾‹å¦‚: "xiÇo" -> ["x", "iÇo"]
              "mÇ" -> ["m", "Ç"]
              "Ã i" -> ["Ã i"]  (é›¶å£°æ¯)
        """
        if not pinyin:
            return []

        # æŒ‰é•¿åº¦ä¼˜å…ˆå°è¯•åŒ¹é…å£°æ¯ï¼ˆzh, ch, shä¼˜å…ˆï¼‰
        for initial in sorted(self.initials, key=len, reverse=True):
            if pinyin.startswith(initial):
                final = pinyin[len(initial):]
                if final:
                    return [initial, final]
                else:
                    return [initial]

        # æ²¡æœ‰å£°æ¯ï¼ˆé›¶å£°æ¯ï¼‰
        return [pinyin]

    def chinese_to_keyword_format(self, chinese_text: str) -> str:
        """
        å°†ä¸­æ–‡è½¬æ¢ä¸ºkeywordæ ¼å¼

        Args:
            chinese_text: ä¸­æ–‡æ–‡æœ¬ï¼Œå¦‚"å°ç±³å°ç±³"

        Returns:
            keywordæ ¼å¼ï¼Œå¦‚"x iÇo m Ç x iÇo m Ç @å°ç±³å°ç±³"
        """
        # è½¬æ¢ä¸ºå¸¦å£°è°ƒæ‹¼éŸ³
        pinyin_list = lazy_pinyin(chinese_text, style=Style.TONE)

        # åˆ†å‰²æ¯ä¸ªæ‹¼éŸ³
        split_parts = []
        missing_tokens = []

        for pinyin in pinyin_list:
            parts = self._split_pinyin(pinyin)

            # éªŒè¯æ¯ä¸ªpartæ˜¯å¦åœ¨tokensä¸­
            for part in parts:
                if part not in self.available_tokens:
                    missing_tokens.append(part)
                split_parts.append(part)

        # æ‹¼æ¥ç»“æœ
        pinyin_str = " ".join(split_parts)
        keyword_line = f"{pinyin_str} @{chinese_text}"

        # å¦‚æœæœ‰ç¼ºå¤±çš„tokenï¼Œç»™å‡ºè­¦å‘Š
        if missing_tokens:
            print(f"âš ï¸  è­¦å‘Š: ä»¥ä¸‹tokenä¸åœ¨tokens.txtä¸­: {', '.join(set(missing_tokens))}")
            print(f"   ç”Ÿæˆçš„å…³é”®è¯å¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œ")

        return keyword_line

    def add_keyword(self, chinese_text: str, append: bool = True) -> bool:
        """
        æ·»åŠ å”¤é†’è¯åˆ°keywords.txt

        Args:
            chinese_text: ä¸­æ–‡å”¤é†’è¯
            append: æ˜¯å¦è¿½åŠ ï¼ˆTrueï¼‰æˆ–è¦†ç›–ï¼ˆFalseï¼‰

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            # ç”Ÿæˆkeywordæ ¼å¼
            keyword_line = self.chinese_to_keyword_format(chinese_text)

            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            if self.keywords_file.exists():
                with open(self.keywords_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    if f"@{chinese_text}" in content:
                        print(f"âš ï¸  å…³é”®è¯ '{chinese_text}' å·²å­˜åœ¨")
                        return False

            # å†™å…¥æ–‡ä»¶
            mode = 'a' if append else 'w'
            with open(self.keywords_file, mode, encoding='utf-8') as f:
                f.write(keyword_line + '\n')

            print(f"âœ… æˆåŠŸæ·»åŠ : {keyword_line}")
            return True

        except Exception as e:
            print(f"âŒ æ·»åŠ å¤±è´¥: {e}")
            return False

    def batch_add_keywords(self, chinese_texts: list, overwrite: bool = False):
        """
        æ‰¹é‡æ·»åŠ å”¤é†’è¯

        Args:
            chinese_texts: ä¸­æ–‡åˆ—è¡¨
            overwrite: æ˜¯å¦è¦†ç›–åŸæ–‡ä»¶
        """
        if overwrite:
            print("âš ï¸  å°†è¦†ç›–ç°æœ‰keywords.txt")

        success_count = 0
        for text in chinese_texts:
            text = text.strip()
            if not text:
                continue

            if self.add_keyword(text, append=not overwrite):
                success_count += 1

            # ç¬¬ä¸€ä¸ªåéƒ½è¿½åŠ 
            overwrite = False

        print(f"\nğŸ“Š å®Œæˆ: æˆåŠŸæ·»åŠ  {success_count}/{len(chinese_texts)} ä¸ªå…³é”®è¯")

    def list_keywords(self):
        """åˆ—å‡ºå½“å‰æ‰€æœ‰å…³é”®è¯"""
        if not self.keywords_file.exists():
            print("âš ï¸  keywords.txt ä¸å­˜åœ¨")
            return

        print(f"\nğŸ“„ å½“å‰å…³é”®è¯åˆ—è¡¨ ({self.keywords_file}):")
        print("-" * 60)

        with open(self.keywords_file, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f, 1):
                line = line.strip()
                if line and not line.startswith('#'):
                    # æå–ä¸­æ–‡éƒ¨åˆ†æ˜¾ç¤º
                    if '@' in line:
                        pinyin_part, chinese_part = line.split('@', 1)
                        print(f"{i}. {chinese_part.strip():15s} -> {pinyin_part.strip()}")
                    else:
                        print(f"{i}. {line}")

        print("-" * 60)


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(
        description="å”¤é†’è¯è‡ªåŠ¨ç”Ÿæˆå·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # æ·»åŠ å•ä¸ªå…³é”®è¯
  python keyword_generator.py -a "å°ç±³å°ç±³"

  # æ‰¹é‡æ·»åŠ å…³é”®è¯
  python keyword_generator.py -b "å°ç±³å°ç±³" "ä½ å¥½å°æ™º" "è´¾ç»´æ–¯"

  # ä»æ–‡ä»¶æ‰¹é‡å¯¼å…¥ï¼ˆæ¯è¡Œä¸€ä¸ªä¸­æ–‡ï¼‰
  python keyword_generator.py -f keywords_input.txt

  # åˆ—å‡ºå½“å‰å…³é”®è¯
  python keyword_generator.py -l

  # æµ‹è¯•è½¬æ¢ï¼ˆä¸å†™å…¥æ–‡ä»¶ï¼‰
  python keyword_generator.py -t "å°ç±³å°ç±³"
        """
    )

    parser.add_argument(
        '-m', '--model-dir',
        default='models',
        help='æ¨¡å‹ç›®å½•è·¯å¾„ï¼ˆé»˜è®¤: modelsï¼‰'
    )

    parser.add_argument(
        '-a', '--add',
        help='æ·»åŠ å•ä¸ªå…³é”®è¯ï¼ˆä¸­æ–‡ï¼‰'
    )

    parser.add_argument(
        '-b', '--batch',
        nargs='+',
        help='æ‰¹é‡æ·»åŠ å…³é”®è¯ï¼ˆå¤šä¸ªä¸­æ–‡ï¼Œç©ºæ ¼åˆ†éš”ï¼‰'
    )

    parser.add_argument(
        '-f', '--file',
        help='ä»æ–‡ä»¶æ‰¹é‡å¯¼å…¥ï¼ˆæ¯è¡Œä¸€ä¸ªä¸­æ–‡ï¼‰'
    )

    parser.add_argument(
        '-l', '--list',
        action='store_true',
        help='åˆ—å‡ºå½“å‰æ‰€æœ‰å…³é”®è¯'
    )

    parser.add_argument(
        '-t', '--test',
        help='æµ‹è¯•è½¬æ¢ï¼ˆä¸å†™å…¥æ–‡ä»¶ï¼‰'
    )

    parser.add_argument(
        '--overwrite',
        action='store_true',
        help='è¦†ç›–æ¨¡å¼ï¼ˆæ¸…ç©ºç°æœ‰å…³é”®è¯ï¼‰'
    )

    args = parser.parse_args()

    # ç¡®å®šæ¨¡å‹ç›®å½•
    if Path(args.model_dir).is_absolute():
        model_dir = Path(args.model_dir)
    else:
        # ç›¸å¯¹è·¯å¾„ï¼šç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•
        script_dir = Path(__file__).parent
        project_root = script_dir.parent
        model_dir = project_root / args.model_dir

    if not model_dir.exists():
        print(f"âŒ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {model_dir}")
        sys.exit(1)

    print(f"ğŸ”§ ä½¿ç”¨æ¨¡å‹ç›®å½•: {model_dir}")

    # åˆ›å»ºç”Ÿæˆå™¨
    generator = KeywordGenerator(model_dir)

    # æ‰§è¡Œæ“ä½œ
    if args.test:
        # æµ‹è¯•æ¨¡å¼
        print(f"\nğŸ§ª æµ‹è¯•è½¬æ¢:")
        keyword_line = generator.chinese_to_keyword_format(args.test)
        print(f"   è¾“å…¥: {args.test}")
        print(f"   è¾“å‡º: {keyword_line}")

    elif args.add:
        # æ·»åŠ å•ä¸ª
        generator.add_keyword(args.add)

    elif args.batch:
        # æ‰¹é‡æ·»åŠ 
        generator.batch_add_keywords(args.batch, overwrite=args.overwrite)

    elif args.file:
        # ä»æ–‡ä»¶å¯¼å…¥
        input_file = Path(args.file)
        if not input_file.exists():
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
            sys.exit(1)

        with open(input_file, 'r', encoding='utf-8') as f:
            keywords = [line.strip() for line in f if line.strip()]

        print(f"ğŸ“¥ ä»æ–‡ä»¶å¯¼å…¥ {len(keywords)} ä¸ªå…³é”®è¯")
        generator.batch_add_keywords(keywords, overwrite=args.overwrite)

    elif args.list:
        # åˆ—å‡ºå…³é”®è¯
        generator.list_keywords()

    else:
        # äº¤äº’æ¨¡å¼
        print("\nğŸ¤ å”¤é†’è¯ç”Ÿæˆå·¥å…·ï¼ˆäº¤äº’æ¨¡å¼ï¼‰")
        print("è¾“å…¥ä¸­æ–‡å”¤é†’è¯ï¼ŒæŒ‰ Ctrl+C æˆ–è¾“å…¥ 'q' é€€å‡º\n")

        try:
            while True:
                chinese = input("è¯·è¾“å…¥ä¸­æ–‡å”¤é†’è¯: ").strip()

                if not chinese or chinese.lower() == 'q':
                    break

                generator.add_keyword(chinese)
                print()

        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å·²é€€å‡º")

    # æœ€ååˆ—å‡ºæ‰€æœ‰å…³é”®è¯
    if not args.list and (args.add or args.batch or args.file):
        generator.list_keywords()


if __name__ == "__main__":
    main()
